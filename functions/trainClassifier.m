function [trainedClassifier, validationAccuracy, validationPredictions, validationScores, kfold_accuracies, kfold_auc, AUC] = ...
        trainClassifier(trainingData, predictor_names, class_names)
    % [trainedClassifier, validationAccuracy] = trainClassifier(trainingData)
    % returns a trained classifier and its accuracy. This code recreates the
    % classification model trained in Classification Learner app. Use the
    % generated code to automate training the same model with new data, or to
    % learn how to programmatically train models.
    %
    %  Input:
    %      trainingData: a table containing the same predictor and response
    %       columns as imported into the app.
    %
    %  Output:
    %      trainedClassifier: a struct containing the trained classifier. The
    %       struct contains various fields with information about the trained
    %       classifier.
    %
    %      trainedClassifier.predictFcn: a function to make predictions on new
    %       data.
    %
    %      validationAccuracy: a double containing the accuracy in percent. In
    %       the app, the History list displays this overall accuracy score for
    %       each model.
    %
    % Use the code to train the model with new data. To retrain your
    % classifier, call the function from the command line with your original
    % data or new data as the input argument trainingData.
    %
    % For example, to retrain a classifier trained with the original data set
    % T, enter:
    %   [trainedClassifier, validationAccuracy] = trainClassifier(T)
    %
    % To make predictions with the returned 'trainedClassifier' on new data T2,
    % use
    %   yfit = trainedClassifier.predictFcn(T2)
    %
    % T2 must be a table containing at least the same predictor columns as used
    % during training. For details, enter:
    %   trainedClassifier.HowToPredict
    
    % Auto-generated by MATLAB on 31-Jul-2019 14:13:20
    
    
    % Extract predictors and response
    % This code processes the data into the right shape for training the
    % model.
    inputTable = trainingData;
    predictorNames = predictor_names;
    predictors = inputTable(:, predictorNames);
    response = inputTable.Label;
    isCategoricalPredictor = repmat(false, 1, length(predictorNames));
    
    % Apply a PCA to the predictor matrix.
    % Run PCA on numeric predictors only. Categorical predictors are passed through PCA untouched.
    isCategoricalPredictorBeforePCA = isCategoricalPredictor;
    numericPredictors = predictors(:, ~isCategoricalPredictor);
    numericPredictors = table2array(varfun(@double, numericPredictors));
    % 'inf' values have to be treated as missing data for PCA.
    numericPredictors(isinf(numericPredictors)) = NaN;
    [pcaCoefficients, pcaScores, ~, ~, explained, pcaCenters] = pca(...
        numericPredictors);
    % Keep enough components to explain the desired amount of variance.
    explainedVarianceToKeepAsFraction = 99/100;
    numComponentsToKeep = find(cumsum(explained)/sum(explained) >= explainedVarianceToKeepAsFraction, 1);
    pcaCoefficients = pcaCoefficients(:,1:numComponentsToKeep);
    predictors = [array2table(pcaScores(:,1:numComponentsToKeep)), predictors(:, isCategoricalPredictor)];
    isCategoricalPredictor = [false(1,numComponentsToKeep), true(1,sum(isCategoricalPredictor))];
    
    % Train a classifier
    % This code specifies all the classifier options and trains the classifier.
    classificationDiscriminant = fitcdiscr(...
        predictors, ...
        response, ...
        'DiscrimType', 'linear', ...
        'Gamma', 0, ...
        'FillCoeffs', 'on', ...
        'ClassNames', [class_names(1,:); class_names(2,:)]);
    
    % Create the result struct with predict function
    predictorExtractionFcn = @(t) t(:, predictorNames);
    pcaTransformationFcn = @(x) [ array2table((table2array(varfun(@double, x(:, ~isCategoricalPredictorBeforePCA))) - pcaCenters) * pcaCoefficients), x(:,isCategoricalPredictorBeforePCA) ];
    discriminantPredictFcn = @(x) predict(classificationDiscriminant, x);
    trainedClassifier.predictFcn = @(x) discriminantPredictFcn(pcaTransformationFcn(predictorExtractionFcn(x)));
    
    % Add additional fields to the result struct
    trainedClassifier.RequiredVariables = predictor_names;
    trainedClassifier.PCACenters = pcaCenters;
    trainedClassifier.PCACoefficients = pcaCoefficients;
    trainedClassifier.ClassificationDiscriminant = classificationDiscriminant;
    trainedClassifier.About = 'This struct is a trained model exported from Classification Learner R2017b.';
    trainedClassifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  yfit = c.predictFcn(T) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedModel''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');
    
    % Extract predictors and response
    % This code processes the data into the right shape for training the
    % model.
    inputTable = trainingData;
    predictorNames = predictor_names;
    predictors = inputTable(:, predictorNames);
    response = inputTable.Label;
    isCategoricalPredictor = repmat(false, 1, length(predictorNames));
    
    % Perform cross-validation
    KFolds = 10;
    cvp = cvpartition(response, 'KFold', KFolds);
    % Initialize the predictions to the proper sizes
    validationPredictions = response;
    numObservations = size(predictors, 1);
    numClasses = 2;
    validationScores = NaN(numObservations, numClasses);
    for fold = 1:KFolds
        trainingPredictors = predictors(cvp.training(fold), :);
        trainingResponse = response(cvp.training(fold), :);
        foldIsCategoricalPredictor = isCategoricalPredictor;
        
        % Apply a PCA to the predictor matrix.
        % Run PCA on numeric predictors only. Categorical predictors are passed through PCA untouched.
        isCategoricalPredictorBeforePCA = foldIsCategoricalPredictor;
        numericPredictors = trainingPredictors(:, ~foldIsCategoricalPredictor);
        numericPredictors = table2array(varfun(@double, numericPredictors));
        % 'inf' values have to be treated as missing data for PCA.
        numericPredictors(isinf(numericPredictors)) = NaN;
        [pcaCoefficients, pcaScores, ~, ~, explained, pcaCenters] = pca(numericPredictors);
        % Keep enough components to explain the desired amount of variance.
        explainedVarianceToKeepAsFraction = 99/100;
        numComponentsToKeep = find(cumsum(explained)/sum(explained) >= explainedVarianceToKeepAsFraction, 1);
        pcaCoefficients = pcaCoefficients(:,1:numComponentsToKeep);
        trainingPredictors = [array2table(pcaScores(:,1:numComponentsToKeep)), trainingPredictors(:, foldIsCategoricalPredictor)];
        foldIsCategoricalPredictor = [false(1,numComponentsToKeep), true(1,sum(foldIsCategoricalPredictor))];
        
        % Train a classifier
        % This code specifies all the classifier options and trains the classifier.
        classificationDiscriminant = fitcdiscr(...
            trainingPredictors, ...
            trainingResponse, ...
            'DiscrimType', 'linear', ...
            'Gamma', 0, ...
            'FillCoeffs', 'off', ...
            'ClassNames', [class_names(1,:); class_names(2,:)]);
        
        % Create the result struct with predict function
        pcaTransformationFcn = @(x) [ array2table((table2array(varfun(@double, x(:, ~isCategoricalPredictorBeforePCA))) - pcaCenters) * pcaCoefficients), x(:,isCategoricalPredictorBeforePCA) ];
        discriminantPredictFcn = @(x) predict(classificationDiscriminant, x);
        validationPredictFcn = @(x) discriminantPredictFcn(pcaTransformationFcn(x));
        
        % Add additional fields to the result struct
        
        % Compute validation predictions
        validationPredictors = predictors(cvp.test(fold), :);
        [foldPredictions, foldScores] = validationPredictFcn(validationPredictors);
        
        % Store predictions in the original order
        validationPredictions(cvp.test(fold), :) = foldPredictions;
        validationScores(cvp.test(fold), :) = foldScores;
       
        %% compute performance for every iteration
        % check which predictions match the responses 
        correctPredictions = all(foldPredictions == response(cvp.test(fold),:), 2); 
        % and compute accuracy 
        validationAccuracy = sum(correctPredictions)/length(correctPredictions);
        kfold_accuracies(fold) = validationAccuracy;
        
        % also compute AUC
        [~,~,~,kfold_auc(fold),~]  = perfcurve(response(cvp.test(fold),:), foldScores(:,2), class_names(2,:));

    end
    
    % Compute validation accuracy
    % TP + TN
    correctPredictions = all(validationPredictions == response, 2);
    isMissing = all(isspace(response), 2);
    correctPredictions = correctPredictions(~isMissing, :);
    % (TP + TN) / (TP+TN+FP+FN) (--> all)
    validationAccuracy = sum(correctPredictions)/length(response); %/length(correctPredictions): this was sensitivity, not accuracy
    disp(['all = ', num2str(mean(kfold_accuracies)), ' validationAcc = ', num2str(validationAccuracy)]);
    
    % get overall AUC in addition to accuracy
    [X,Y,~,AUC,~]  = perfcurve(response, validationScores(:,2), class_names(2,:));
    


    